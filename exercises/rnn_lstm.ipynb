{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    # Set seed for Python's random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Set seed for CUDA (if using)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "        \n",
    "    # Make PyTorch deterministic (this can slow down the computation)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Example of usage\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "{'input_ids': [50256], 'attention_mask': [1]}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(tokenizer.pad_token)\n",
    "    print(tokenizer(tokenizer.pad_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_pattern = re.compile(r\",|the|a\", flags=re.IGNORECASE)\n",
    "\n",
    "def preprocess(batch: list[str]) -> list[str]:\n",
    "    result = [cleanup_pattern.sub(\"\", x) for x in batch]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40])\n",
      "torch.Size([3, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[12804,  7586, 21831, 18045,   625,   220,   300,  7357,  3290, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 2396,  5891,  4290,  1309,   514,   407,   307,  7770,   284,   674,\n",
       "           5400,   532,   475,  1309,   514,   300,   568,  1277,   256,    83,\n",
       "           1463,   284,   674,  2219,  5353,   299,    67,   284,   220,   285,\n",
       "            641,   416,   543,   883,  5400,   269,    77,   307, 12939,    13],\n",
       "         [   37,  5037,  4017,   291,  5907,  1341,   407,   348,    83,   534,\n",
       "           1499,   269,    77,   466,   329,   345,  1341,   348,    83,   345,\n",
       "            269,    77,   466,   329,   534,  1499,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch: list[str], max_length: int | None = None):\n",
    "    batch = preprocess(batch)\n",
    "    encodings = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encodings.input_ids.to(device),\n",
    "        \"attention_mask\": encodings.attention_mask.to(device),\n",
    "    }\n",
    "\n",
    "\n",
    "res = tokenize([\n",
    "    \"Big brown fox jumps over the lazy dog\",\n",
    "    \"So, fellow citizens, let us not be blind to our differences - but let us also direct attention to our common interests and to the means by which those differences can be resolved.\",\n",
    "    \"Fellow americans, ask not what your country can do for you, ask what you can do for your country.\",\n",
    "])\n",
    "print(res['input_ids'].shape)\n",
    "print(res['attention_mask'].shape)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples: list[dict], max_length: int | None = None):\n",
    "    text = [x['text'] for x in examples]\n",
    "    return tokenize(text, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 20\n",
    "max_length = 100\n",
    "embed_dim = 300\n",
    "state_dim = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e997081614194572b1b97ad62491b278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b4932416c748948fbe69b9224ff8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6df2c3680c46a9910e503bde203bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"allenai/c4\", \"realnewslike\")\n",
    "train_subset = Subset(dataset[\"train\"], torch.arange(10000))\n",
    "val_subset = Subset(dataset[\"validation\"], torch.arange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in an epoch: 500\n",
      "input_ids shape: torch.Size([20, 100])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=partial(collate_fn, max_length=max_length),\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=partial(collate_fn, max_length=max_length),\n",
    ")\n",
    "print(\"Number of batches in an epoch:\", len(train_loader))\n",
    "for batch in train_loader:\n",
    "    print(\"input_ids shape:\", batch['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.rnn.LSTM"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/pytorch/pytorch/blob/aa7be72cc55244978ddaf760338dab6b9cf977a1/torch/nn/modules/rnn.py#L631\n",
    "torch.nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.5257, Val Loss: 5.8510\n",
      "Epoch 2, Train Loss: 5.5259, Val Loss: 5.4889\n",
      "Epoch 3, Train Loss: 5.1310, Val Loss: 5.3271\n",
      "Epoch 4, Train Loss: 4.8439, Val Loss: 5.2432\n",
      "Epoch 5, Train Loss: 4.6047, Val Loss: 5.2070\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, state_dim: int, max_length: int | None = None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, state_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(state_dim, vocab_size)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        hidden: torch.Tensor | None = None,\n",
    "    ):\n",
    "        x = self.embedding(x)\n",
    "        x = x * attention_mask.unsqueeze(-1)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    # See: https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc\n",
    "    @torch.inference_mode\n",
    "    def predict(self, input_text):\n",
    "        tokens = tokenize([input_text], max_length=self.max_length)\n",
    "        outputs = self(tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        next_word = tokenizer.decode(preds[0][-1], skip_special_tokens=True)\n",
    "        return next_word.strip()\n",
    "\n",
    "\n",
    "model = LSTMModel(vocab_size=tokenizer.vocab_size, embed_dim=embed_dim, state_dim=state_dim).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Adam has many hyperparameters - play with it!\n",
    "# Try other optimizers as well!\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "# NOTE: always track the validation accuracy to see when it diverges from the training accuracy!\n",
    "# This is a sign of overfitting!\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"][:, :-1]\n",
    "        targets = batch[\"input_ids\"][:, 1:]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask[:, :-1])\n",
    "        outputs = outputs * attention_mask[:, 1:].unsqueeze(-1)\n",
    "        loss = criterion(outputs.reshape(-1, tokenizer.vocab_size), targets.reshape(-1))\n",
    "        # loss must be a scalar for loss.backward() to work!\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"][:, :-1]\n",
    "            targets = batch[\"input_ids\"][:, 1:]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask[:, :-1])\n",
    "            outputs = outputs * attention_mask[:, 1:].unsqueeze(-1)\n",
    "            loss = criterion(outputs.reshape(-1, tokenizer.vocab_size), targets.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}\"\n",
    "    )\n",
    "\n",
    "# model.predict(\"Big brown fox jumps over the lazy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'been'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Austria has\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austria has been \n",
      "Austria has been  first\n",
      "Austria has been  first time\n",
      "Austria has been  first time of\n",
      "Austria has been  first time of \n",
      "Austria has been  first time of  l\n",
      "Austria has been  first time of  l st\n",
      "Austria has been  first time of  l st te\n",
      "Austria has been  first time of  l st te chers\n",
      "Austria has been  first time of  l st te chers of\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"\"]\n",
    "for i in range(10):\n",
    "    next_word = model.predict(\" \".join(prompt))\n",
    "    prompt.append(next_word)\n",
    "    print(\" \".join(prompt))\n",
    "    if next_word == \".\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

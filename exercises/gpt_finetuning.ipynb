{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/distilbert/distilgpt2\n",
    "# https://huggingface.co/openai-community/gpt2/tree/main\n",
    "model_id = \"distilbert/distilgpt2\"\n",
    "tokenizer_id = model_id\n",
    "\n",
    "# https://huggingface.co/datasets/salgara/Grimes_tales\n",
    "dataset_id = \"salgara/Grimes_tales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_id)[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HuggingFace Course: Tokenizers](https://huggingface.co/learn/nlp-course/en/chapter2/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A whitespace-before-the-word and a whitespace symbol\n",
    "tokenizer.vocab[\"Ġ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A newline symbol\n",
    "tokenizer.vocab[\"Ċ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different tokens will the word \"study\" and derived get?\n",
    "nonsense = \"Study or not to study your studies? Studying is light, but not studying is darkness.\"\n",
    "tokens = tokenizer.tokenize(nonsense)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token, Token ID\")\n",
    "for token, token_id in zip(tokens, token_ids):\n",
    "    print(f\"{token:<10} {token_id}\")\n",
    "\n",
    "# Read some more: https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer(nonsense)[\"input_ids\"]\n",
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Story\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/en/chapter7/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized1 = tokenizer(dataset[\"Story\"][:3])\n",
    "print(tokenized1.keys())\n",
    "print(\"Length:\", [len(x) for x in tokenized1[\"input_ids\"]])\n",
    "print(\"Total tokens:\", sum(len(x) for x in tokenized1[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tokenizer() parameters](https://huggingface.co/docs/transformers/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized2 = tokenizer(dataset[\"Story\"][:3], truncation=True, max_length=500, return_overflowing_tokens=True)\n",
    "print(tokenized2.keys())\n",
    "print(\"Length:\", [len(x) for x in tokenized2[\"input_ids\"]])\n",
    "print(\"Total tokens:\", sum(len(x) for x in tokenized2[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized3 = tokenizer(dataset[\"Story\"][:3], truncation=True, max_length=500, return_overflowing_tokens=True, padding=\"max_length\")\n",
    "print(tokenized3.keys())\n",
    "print(\"Length:\", [len(x) for x in tokenized3[\"input_ids\"]])\n",
    "print(\"Total tokens:\", sum(len(x) for x in tokenized3[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[y for x in tokenized1[\"input_ids\"] for y in x] == [y for x in tokenized2[\"input_ids\"] for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[y for x in tokenized1[\"input_ids\"] for y in x] == [y for x in tokenized3[\"input_ids\"] for y in x if y != tokenizer.pad_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer(\n",
    "    dataset[\"Story\"][:10],\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"input_ids\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"attention_mask\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_tensors=\"pt\" doesn't work with .map()!\n",
    "# https://discuss.huggingface.co/t/dataset-map-return-only-list-instead-torch-tensors/15767\n",
    "# Use ds.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True) after .map()\n",
    "\n",
    "def tokenize_batch(examples):\n",
    "    print(\"Number of examples:\", len(examples[\"Story\"]))\n",
    "\n",
    "    res = tokenizer(\n",
    "        examples[\"Story\"],\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding=\"max_length\",  # Defaults to the max length of the model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    print(\"Result shape:\", res[\"input_ids\"].shape)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPOILER ALERT! DO NOT SCROLL FURTHER DOWN! UNCOMMENT THE FOLLOWING LINE AND FIX THE ERROR!\n",
    "# dataset.map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Story\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Use remove_columns to drop the columns that don't have the same number of rows as the tokenized columns\n",
    "# https://discuss.huggingface.co/t/how-to-use-map-or-similar-when-one-row-is-mapped-to-multiple-rows/8374\n",
    "\n",
    "train_dataset = ds[\"train\"].map(tokenize_batch, remove_columns=dataset.column_names, batched=True)\n",
    "train_dataset.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "print(train_dataset)\n",
    "test_dataset = ds[\"test\"].map(tokenize_batch, remove_columns=dataset.column_names, batched=True)\n",
    "test_dataset.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"input_ids\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HuggingFace: Causal language modeling](https://huggingface.co/docs/transformers/en/tasks/language_modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HuggingFace course: Fine-tune a pretrained model](https://huggingface.co/docs/transformers/en/training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[:2][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can send inputs shorter than the model's context lengths (1024)\n",
    "res = model(train_dataset[\"input_ids\"][0][:100].to(device))\n",
    "tokenizer.decode(res.logits.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = coll([train_dataset[:2]])\n",
    "\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "assert torch.allclose(out[\"input_ids\"], train_dataset[:2][\"input_ids\"])\n",
    "\n",
    "# Note that labels are the same as input_ids\n",
    "assert torch.allclose(out[\"input_ids\"], out[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/karpathy/tiny_shakespeare\n",
    "shakespeare_dataset_id = \"karpathy/tiny_shakespeare\"\n",
    "\n",
    "shakespeare = load_dataset(shakespeare_dataset_id)\n",
    "\n",
    "def tokenize_batch_shakespeare(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        padding=\"max_length\",  # Defaults to the max length of the model\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "shakespeare_train = shakespeare[\"train\"].map(tokenize_batch_shakespeare, remove_columns=[\"text\"], batched=True)\n",
    "shakespeare_train.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "shakespeare_val = shakespeare[\"validation\"].map(tokenize_batch_shakespeare, remove_columns=[\"text\"], batched=True)\n",
    "shakespeare_val.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n",
    "shakespeare_test = shakespeare[\"test\"].map(tokenize_batch_shakespeare, remove_columns=[\"text\"], batched=True)\n",
    "shakespeare_test.set_format(\"pt\", columns=[\"input_ids\"], output_all_columns=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_datasets = concatenate_datasets([train_dataset, shakespeare_train])\n",
    "all_test_datasets = concatenate_datasets([test_dataset, shakespeare_val, shakespeare_test])\n",
    "\n",
    "all_train_datasets[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"shakespeare_grim_gpt2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    seed=42,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.001,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=all_train_datasets,\n",
    "    eval_dataset=all_test_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "steps_per_epoch = math.ceil(len(all_train_datasets) / training_args.per_device_train_batch_size)\n",
    "print(\"Total number of training examples:\", len(all_train_datasets))\n",
    "print(\"Number of steps per epoch:\", steps_per_epoch)\n",
    "print(\"Total number of steps:\", steps_per_epoch * training_args.num_train_epochs)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Arthur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(prompt):\n",
    "    print(generator(prompt, max_new_tokens=200)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(f\"A long time ago there lived a king named {name} who was known for his love of outrageous wigs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(f\"There was once a peasant named {name} who owned a cat and was afraid of the big black wolf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
